{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guidelines for Prompting\n",
    "\n",
    "> We will practice two prompting principles and their related tactics in order to write effective prompts for large language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llms_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "\n",
    "- **Principle 1: Write clear, specific instructions with context of the task**\n",
    "- **Principle 2: Give the model time to ‚Äúthink‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tactics\n",
    "\n",
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which may require longer prompts for more clarity and context.\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"title\": \"Introduction to Machine Learning\",\n",
      "    \"lecturer\": \"Dr. John Smith\",\n",
      "    \"description\": \"This workshop will provide an overview of machine learning, including supervised and unsupervised learning, and the different types of algorithms used in machine learning. Participants will also learn how to implement machine learning algorithms using Python.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Natural Language Processing\",\n",
      "    \"lecturer\": \"Dr. Jane Doe\",\n",
      "    \"description\": \"This workshop will cover the basics of natural language processing, including text preprocessing, sentiment analysis, and topic modeling. Participants will also learn how to use popular NLP libraries such as NLTK and spaCy.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Computer Vision\",\n",
      "    \"lecturer\": \"Dr. David Lee\",\n",
      "    \"description\": \"This workshop will introduce participants to computer vision, including image processing, object detection, and image classification. Participants will also learn how to use popular computer vision libraries such as OpenCV and TensorFlow.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Reinforcement Learning\",\n",
      "    \"lecturer\": \"Dr. Sarah Kim\",\n",
      "    \"description\": \"This workshop will provide an introduction to reinforcement learning, including the different types of reinforcement learning algorithms and their applications. Participants will also learn how to implement reinforcement learning algorithms using Python and TensorFlow.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Deep Learning\",\n",
      "    \"lecturer\": \"Dr. Michael Chen\",\n",
      "    \"description\": \"This workshop will cover the basics of deep learning, including neural networks, convolutional neural networks, and recurrent neural networks. Participants will also learn how to use popular deep learning libraries such as Keras and PyTorch.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pl = f\"\"\"\n",
    "PiszƒÖc prompty nalezy wyraziƒá, co model ma robiƒá, dostarczajƒÖc instrukcje, kt√≥re sƒÖ tak jasne i szczeg√≥≈Çowe, jak to tylko mo≈ºliwe.\n",
    "Poprowadzi to model w kierunku po≈ºƒÖdanego efektu i zmniejsza szanse na otrzymanie nieistotnych lub nieprawid≈Çowych odpowiedzi.\n",
    "Nie nale≈ºy myliƒá pisania jasnej podpowiedzi z kr√≥tkƒÖ podpowiedziƒÖ. \n",
    "W wielu przypadkach d≈Çu≈ºsze prompty zapewniajƒÖ wiƒôkszƒÖ jasno≈õƒá i kontekst dla modelu. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jasne i szczeg√≥≈Çowe instrukcje w promptach pomagajƒÖ modelowi osiƒÖgnƒÖƒá po≈ºƒÖdany efekt i uniknƒÖƒá nieistotnych lub nieprawid≈Çowych odpowiedzi.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt_pl = f\"\"\"\n",
    "Podsumuj tekst ograniczony potr√≥jnymi znakami ``` w jedno zdanie.\n",
    "```{text_pl}```\n",
    "\"\"\"\n",
    "response_pl = get_completion(prompt_pl)\n",
    "print(response_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñüí¨ Jasne i szczeg√≥≈Çowe prompty pomagajƒÖ modelowi osiƒÖgnƒÖƒá po≈ºƒÖdany efekt i uniknƒÖƒá b≈Çƒôd√≥w. D≈Çu≈ºsze prompty sƒÖ lepsze ni≈º kr√≥tkie, bo dajƒÖ wiƒôcej kontekstu.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt_pl = f\"\"\"\n",
    "Podsumuj tekst ograniczony potr√≥jnymi znakami ``` w jedno zdanie w stylu nastolatka korzystajƒÖcego z emoji.\n",
    "```{text_pl}```\n",
    "\"\"\"\n",
    "response_pl = get_completion(prompt_pl)\n",
    "print(response_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5' napisz przyk≈Çadowy prompt i sprawd≈∫ czy dzia≈Ça Ci API OpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 2: Ask for a structured output\n",
    "- JSON, HTML, Markdown, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"title\": \"Introduction to Machine Learning\",\n",
      "    \"lecturer\": \"Dr. John Smith\",\n",
      "    \"description\": \"This workshop will provide an overview of machine learning, including supervised and unsupervised learning, and the different types of algorithms used in machine learning. Participants will also learn how to implement machine learning algorithms using Python.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Natural Language Processing\",\n",
      "    \"lecturer\": \"Dr. Jane Doe\",\n",
      "    \"description\": \"This workshop will cover the basics of natural language processing, including text preprocessing, sentiment analysis, and topic modeling. Participants will also learn how to use popular NLP libraries such as NLTK and spaCy.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Computer Vision\",\n",
      "    \"lecturer\": \"Dr. David Lee\",\n",
      "    \"description\": \"This workshop will introduce participants to computer vision, including image processing, object detection, and image classification. Participants will also learn how to use popular computer vision libraries such as OpenCV and TensorFlow.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Reinforcement Learning\",\n",
      "    \"lecturer\": \"Dr. Sarah Kim\",\n",
      "    \"description\": \"This workshop will provide an introduction to reinforcement learning, including the different types of reinforcement learning algorithms and their applications. Participants will also learn how to implement reinforcement learning algorithms using Python.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Deep Learning\",\n",
      "    \"lecturer\": \"Dr. Michael Chen\",\n",
      "    \"description\": \"This workshop will cover the basics of deep learning, including neural networks, convolutional neural networks, and recurrent neural networks. Participants will also learn how to use popular deep learning libraries such as TensorFlow and Keras.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Generate a list of 5 best workshop topics for AI Tech Summer School that is a AI related school.\n",
    "Write title, lecturer name, and description for each topic.\n",
    "Provide them in JSON format with the following keys: \n",
    "title, lecturer, description.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 3: Ask the model to check whether conditions are satisfied - reflect on the task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Choose the topic.\n",
      "Step 2 - Review literature on the chosen topic.\n",
      "Step 3 - Prepare the materials.\n",
      "Step 4 - Prepare the presentation.\n",
      "Step 5 - Deliver the workshop.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text_1 = f\"\"\"\n",
    "Preparation of the workshop is quite easy. However, you must follow some steps. \n",
    "Firstly, you need to choose the topic. Then review literature on this topic. Then prepare the materials.\n",
    "And finally you need to prepare the presentation and deliver the workshop.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple =. \n",
    "If it contains a sequence of steps, re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - \n",
    "Step 2 - \n",
    "Step N - \n",
    "\n",
    "If the text does not contain a sequence of steps, then simply write \\\"No steps are provided.\\\"\n",
    "\n",
    "==={text_1}===\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No steps are provided.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text_2 = \"\"\"\n",
    "Witamy w nowej us≈Çudze Bing\n",
    "Poznaj mo≈ºliwo≈õci obs≈Çugiwanej przez sztucznƒÖ inteligencjƒô funkcji Copilot w Internecie\n",
    "\n",
    "üßê Zadawaj z≈Ço≈ºone pytania\n",
    "\"Jakie posi≈Çki mogƒô przygotowaƒá dla mojego wybrednego malucha, kt√≥ry je tylko jedzenie w kolorze pomara≈Ñczowym?\"\n",
    "\n",
    "üôå Uzyskaj lepsze odpowiedzi\n",
    "\"Jakie sƒÖ zalety i wady 3 najczƒô≈õciej kupowanych odkurzaczy dla zwierzƒÖt domowych?\"\n",
    "\n",
    "üé® ZdobƒÖd≈∫ tw√≥rcze inspiracje\n",
    "\"Napisz wiersz haiku o krokodylach w kosmosie, w kt√≥rym narratorem jest pirat\"\n",
    "Uczmy siƒô razem. Us≈Çuga Bing jest obs≈Çugiwana przez sztucznƒÖ inteligencjƒô, wiƒôc sƒÖ mo≈ºliwe niespodzianki i b≈Çƒôdy. Pamiƒôtaj o sprawdzaniu fakt√≥w oraz przeka≈º opiniƒô, aby≈õmy mogli siƒô uczyƒá i rozwijaƒá!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple =. \n",
    "If it contains a sequence of steps, re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - \n",
    "Step 2 - \n",
    "Step N - \n",
    "\n",
    "If the text does not contain a sequence of steps, then simply write \\\"No steps are provided.\\\"\n",
    "\n",
    "==={text_2}===\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 4: \"Few-shot\" prompting - use a few examples to show to the model how to behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I'm working in Custom Office and we want to synthetically generate data about shipments. Please generate one more examples.\n",
    "###\n",
    "{\n",
    "    \"id\": \"95ea9a1c-f934-4f08-bc74-3ff7c8da5464\",\n",
    "    \"exporter_country_name\": \"Indonesia\",\n",
    "    \"destination_country_name\": \"Portugal\",\n",
    "    \"exporter_country_code\": \"ID\",\n",
    "    \"destination_country_code\": \"PT\",\n",
    "    \"invoice_value\": 36513.02,\n",
    "    \"invoice_currency\": \"EUR\",\n",
    "    \"commodity_code\": 7108110000,\n",
    "    \"weight_gross\": 22.03,\n",
    "    \"weight_net\": 3.08,\n",
    "    \"importer_name\": \"Dawson, Lewis and Miller\",\n",
    "    \"declarant_person\": \"Lydia Reed\",\n",
    "    \"good_description\": \"II. PRECIOUS METALS AND METALS CLAD WITH PRECIOUS METAL -> Gold (including gold plated with platinum), unwrought or in semi-manufactured forms, or in powder form -> Non-monetary -> Powder\",\n",
    "    \"exporter_name\": \"Hurst, Freeman and Kennedy\",\n",
    "    \"origin_country_name\": \"Viet Nam\",\n",
    "    \"origin_country_code\": \"VN\"\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"b3c8d7e2-6f5a-4c5c-9c5d-8f5a9b1c2d4f\",\n",
      "    \"exporter_country_name\": \"China\",\n",
      "    \"destination_country_name\": \"United States\",\n",
      "    \"exporter_country_code\": \"CN\",\n",
      "    \"destination_country_code\": \"US\",\n",
      "    \"invoice_value\": 12000.50,\n",
      "    \"invoice_currency\": \"USD\",\n",
      "    \"commodity_code\": 8517120000,\n",
      "    \"weight_gross\": 45.20,\n",
      "    \"weight_net\": 38.10,\n",
      "    \"importer_name\": \"Smith and Sons\",\n",
      "    \"declarant_person\": \"John Doe\",\n",
      "    \"good_description\": \"VII. VEHICLES, AIRCRAFT, VESSELS AND ASSOCIATED TRANSPORT EQUIPMENT -> Aircraft and associated equipment -> Aircraft engines and parts thereof\",\n",
      "    \"exporter_name\": \"Changzhou Aviation Precision Machinery Co., Ltd.\",\n",
      "    \"origin_country_name\": \"China\",\n",
      "    \"origin_country_code\": \"CN\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"id\": \"f8e9d6c5-4b3a-2c1d-1e0f-9a8b7c6d5e4f\",\n",
      "    \"exporter_country_name\": \"Germany\",\n",
      "    \"destination_country_name\": \"France\",\n",
      "    \"exporter_country_code\": \"DE\",\n",
      "    \"destination_country_code\": \"FR\",\n",
      "    \"invoice_value\": 5000.00,\n",
      "    \"invoice_currency\": \"EUR\",\n",
      "    \"commodity_code\": 3004900000,\n",
      "    \"weight_gross\": 10.50,\n",
      "    \"weight_net\": 8.20,\n",
      "    \"importer_name\": \"Dupont Pharmaceuticals\",\n",
      "    \"declarant_person\": \"Sophie Martin\",\n",
      "    \"good_description\": \"VI. PRODUCTS OF THE CHEMICAL OR ALLIED INDUSTRIES -> Pharmaceutical products -> Medicaments (excluding goods of heading 30.02, 30.05 or 30.06) consisting of mixed or unmixed products for therapeutic or prophylactic uses, put up in measured doses (including those in the form of transdermal administration systems) or in forms or packings for retail sale\",\n",
      "    \"exporter_name\": \"Bayer AG\",\n",
      "    \"origin_country_name\": \"Germany\",\n",
      "    \"origin_country_code\": \"DE\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"id\": \"a1b2c3d4-e5f6-g7h8-i9j0-k1l2m3n4o5p6\",\n",
      "    \"exporter_country_name\": \"Japan\",\n",
      "    \"destination_country_name\": \"Australia\",\n",
      "    \"exporter_country_code\": \"JP\",\n",
      "    \"destination_country_code\": \"AU\",\n",
      "    \"invoice_value\": 75000.00,\n",
      "    \"invoice_currency\": \"AUD\",\n",
      "    \"commodity_code\": 8703230000,\n",
      "    \"weight_gross\": 1500.00,\n",
      "    \"weight_net\": 1200.00,\n",
      "    \"importer_name\": \"Toyota Australia\",\n",
      "    \"declarant_person\": \"David Lee\",\n",
      "    \"good_description\": \"VII. VEHICLES, AIRCRAFT, VESSELS AND ASSOCIATED TRANSPORT EQUIPMENT -> Motor cars and other motor vehicles principally designed for the transport of persons (other than those of heading 87.02), including station wagons and racing cars -> Vehicles specially designed for travelling on snow; golf cars and similar vehicles\",\n",
      "    \"exporter_name\": \"Toyota Motor Corporation\",\n",
      "    \"origin_country_name\": \"Japan\",\n",
      "    \"origin_country_code\": \"JP\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle 2: Give the model time to ‚Äúthink‚Äù \n",
    "\n",
    "#### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåçüìäüëç \"Massively Multilingual Corpus of Sentiment Datasets\" presents a comprehensive collection of 79 datasets in 27 languages for sentiment analysis, along with a multi-faceted sentiment classification benchmark. \n",
      "\n",
      "üìäüåç \"Massively Multilingual Corpus of Sentiment Datasets\" provides a comprehensive collection of datasets in 27 languages for sentiment analysis, along with a multi-faceted sentiment classification benchmark. #sentimentanalysis #multilingual #corpus\n",
      "\n",
      "üåçüìä Looking for a comprehensive collection of sentiment datasets in multiple languages? Check out \"Massively Multilingual Corpus of Sentiment Datasets\" which also includes a multi-faceted sentiment classification benchmark. #sentimentanalysis #multilingual #corpus\n",
      "\n",
      "üëçüåç \"Massively Multilingual Corpus of Sentiment Datasets\" is a valuable resource for sentiment analysis researchers, providing a comprehensive collection of datasets in 27 languages and a multi-faceted sentiment classification benchmark. #sentimentanalysis #multilingual #corpus\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet based on summary.\n",
    "4 - Prepare a linkedin post based on summary. \n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return strcutured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": \"üìö This paper presents a massively multilingual corpus of sentiment datasets consisting of 79 manually selected datasets from over 350 datasets reported in the scientific literature, covering 27 languages representing 6 language families, and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\",\n",
      "    \"title\": \"üìö Massive Multilingual Sentiment Corpus and Benchmark\",\n",
      "    \"tweet\": \"üìö This paper presents a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark covering 27 languages and summarizing hundreds of experiments. #sentimentanalysis #multilingual #corpus\",\n",
      "    \"linkedin_post\": \"Check out this paper presenting a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark covering 27 languages and summarizing hundreds of experiments. This is a significant contribution to the development of large-scale deployments of multilingual models, particularly in the area of multilingual sentiment analysis. #sentimentanalysis #multilingual #corpus #research\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet based on summary.\n",
    "4 - Prepare a linkedin post based on summary. \n",
    "\n",
    "Return a python dictionary with the following keys: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"\"\"\n",
    "Question:\n",
    "I'm organizing a conference that will need a space for up to 1000 people. \n",
    "- Renting the space costs $500/day\n",
    "- Catering the conference costs $50/person\n",
    "- I'll need to hire 3 staff members for the duration of the conference at $75/day/person\n",
    "What is the total cost for the conference as a function of the number of people attending?\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the number of people attending the conference.\n",
    "Costs:\n",
    "1. Space rental cost: 500\n",
    "2. Catering cost: 50\n",
    "3. Staff cost: 75 \n",
    "Total cost: 500 + 50x + 75x = 620x + 500\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's solution is correct.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "{question}\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Solution:\n",
      "Let x be the number of people attending the conference.\n",
      "Costs:\n",
      "1. Space rental cost: 500\n",
      "2. Catering cost: 50x\n",
      "3. Staff cost: 3 * 75 * number of days of conference\n",
      "Total cost: 500 + 50x + 3 * 75 * number of days of conference\n",
      "\n",
      "Difference:\n",
      "The student's solution assumes that the staff cost is a fixed cost of $75 per person per day, regardless of the number of days of the conference. However, the staff cost should be dependent on the number of days of the conference. My solution takes this into account by multiplying the number of days of the conference by the number of staff members and the daily rate of $75.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution is correct or not.\n",
    "\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Write down your steps and highlight the differences between your solution and the student's solution.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Limitations and Problem: \n",
    "\n",
    "### Hallucinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Autonomous Research Assistant is a revolutionary new product developed by the Wroc≈Çaw University of Science and Technology. This cutting-edge technology is designed to assist researchers in their work by automating many of the tedious and time-consuming tasks that are typically associated with research.\n",
      "\n",
      "The Autonomous Research Assistant is a sophisticated machine learning system that is capable of analyzing vast amounts of data and identifying patterns and trends that would be difficult or impossible for a human researcher to detect. It can also generate reports and summaries of its findings, making it easy for researchers to quickly and easily understand the results of their experiments.\n",
      "\n",
      "One of the key features of the Autonomous Research Assistant is its ability to learn and adapt over time. As it analyzes more data and gains more experience, it becomes increasingly accurate and efficient, allowing researchers to focus on more complex and challenging tasks.\n",
      "\n",
      "Another important aspect of the Autonomous Research Assistant is its user-friendly interface. Researchers can easily interact with the system using natural language commands, making it easy to get the information they need quickly and efficiently.\n",
      "\n",
      "Overall, the Autonomous Research Assistant is a game-changing product that has the potential to revolutionize the way research is conducted. By automating many of the tedious and time-consuming tasks associated with research, it allows researchers to focus on more important and challenging work, ultimately leading to faster and more accurate results.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Write me about a new product called \"The Autonomous Research Assistant\" created by Wroc≈Çaw University of Science and Technology.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alba≈Ñski Wordnet to system leksykalny, kt√≥ry zosta≈Ç stworzony przez d≈Çugow≈Çosego wojownika z kraj√≥w ba≈Çka≈Ñskich o imieniu Artan. Artan by≈Ç z pochodzenia Alba≈Ñczykiem i mia≈Ç g≈ÇƒôbokƒÖ wiedzƒô na temat jƒôzyka alba≈Ñskiego oraz jego zwiƒÖzk√≥w z innymi jƒôzykami ba≈Çka≈Ñskimi. W oparciu o swojƒÖ wiedzƒô i do≈õwiadczenie, Artan stworzy≈Ç Alba≈Ñski Wordnet, kt√≥ry jest narzƒôdziem s≈Çu≈ºƒÖcym do analizy semantycznej jƒôzyka alba≈Ñskiego. System ten zawiera wiele informacji na temat znacze≈Ñ s≈Ç√≥w, ich synonim√≥w, antonim√≥w oraz zwiƒÖzk√≥w semantycznych miƒôdzy nimi. Dziƒôki temu Alba≈Ñski Wordnet jest bardzo przydatnym narzƒôdziem dla lingwist√≥w, badaczy jƒôzyka oraz t≈Çumaczy.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Opisz Alba≈Ñski Wordnet wskazujƒÖc na to jaki d≈Çugow≈Çosy wojownik z kraj√≥w ba≈Çka≈Ñskich go stworzy≈Ç?\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
