{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guidelines for Prompting\n",
    "\n",
    "> We will practice two prompting principles and their related tactics in order to write effective prompts for large language models.\n",
    "\n",
    "- title-block-banner: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use google colab please install the following packages:\n",
    "\n",
    "```bash\n",
    "!pip install \"panel>=1.1.0\" \"tiktoken>=0.3.3\" \"openai>=0.27.8\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use .env files to load openai key or pass it directly in jupyter. \n",
    "\n",
    "Please rememeber to remove the key from notebook before pushing to git or remove the key in OpenAI settings. \n",
    "\n",
    "```warning\n",
    "I added to the repository keys from OpenIA (they have been already disabled, but it is a nice exercise to find them in the git history)\n",
    "\n",
    "Good Luck :)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to usse the .env file you could need to load enbironment variables with dotenv library \n",
    "# !pip install python-dotenv\n",
    "\n",
    "# import os\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "# openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwie you can use the following code to pass the key directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# openai.api_key = \"sk-....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "#     print(str(response.choices[0].message))\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "\n",
    "- **Principle 1: Write clear, specific instructions with context of the task**\n",
    "- **Principle 2: Give the model time to ‚Äúthink‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tactics\n",
    "\n",
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which may require longer prompts for more clarity and context.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pl = f\"\"\"\n",
    "PiszƒÖc prompty nalezy wyraziƒá, co model ma robiƒá, dostarczajƒÖc instrukcje, kt√≥re sƒÖ tak jasne i szczeg√≥≈Çowe, jak to tylko mo≈ºliwe.\n",
    "Poprowadzi to model w kierunku po≈ºƒÖdanego efektu i zmniejsza szanse na otrzymanie nieistotnych lub nieprawid≈Çowych odpowiedzi.\n",
    "Nie nale≈ºy myliƒá pisania jasnej podpowiedzi z kr√≥tkƒÖ podpowiedziƒÖ. \n",
    "W wielu przypadkach d≈Çu≈ºsze prompty zapewniajƒÖ wiƒôkszƒÖ jasno≈õƒá i kontekst dla modelu. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jasne i szczeg√≥≈Çowe instrukcje w promptach pomagajƒÖ modelowi osiƒÖgnƒÖƒá po≈ºƒÖdany efekt i uniknƒÖƒá nieistotnych lub nieprawid≈Çowych odpowiedzi.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt_pl = f\"\"\"\n",
    "Podsumuj tekst ograniczony potr√≥jnymi znakami ``` w jedno zdanie.\n",
    "```{text_pl}```\n",
    "\"\"\"\n",
    "response_pl = get_completion(prompt_pl)\n",
    "print(response_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñüí¨ Jasne i szczeg√≥≈Çowe prompty pomagajƒÖ modelowi osiƒÖgnƒÖƒá po≈ºƒÖdany efekt, wiƒôc nie bƒÖd≈∫ leniwy i pisz d≈Çu≈ºsze podpowiedzi! üìùüëç\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt_pl = f\"\"\"\n",
    "Podsumuj tekst ograniczony potr√≥jnymi znakami ``` w jedno zdanie w stylu nastolatka korzystajƒÖcego z emoji.\n",
    "```{text_pl}```\n",
    "\"\"\"\n",
    "response_pl = get_completion(prompt_pl)\n",
    "print(response_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 2: Ask for a structured output\n",
    "- JSON, HTML, Markdown, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Title | Lecturer | Description |\n",
      "| --- | --- | --- |\n",
      "| Introduction to Machine Learning | John Smith | This workshop will provide an overview of machine learning, including supervised and unsupervised learning, and the basics of neural networks. Participants will learn how to build and train a simple machine learning model using Python. |\n",
      "| Natural Language Processing | Jane Doe | This workshop will cover the basics of natural language processing (NLP), including text preprocessing, sentiment analysis, and named entity recognition. Participants will learn how to use popular NLP libraries such as NLTK and spaCy to analyze text data. |\n",
      "| Computer Vision | David Lee | This workshop will introduce participants to computer vision, including image classification, object detection, and image segmentation. Participants will learn how to use popular computer vision libraries such as OpenCV and TensorFlow to build and train models. |\n",
      "| Reinforcement Learning | Sarah Kim | This workshop will cover the basics of reinforcement learning, including Markov decision processes, Q-learning, and policy gradients. Participants will learn how to build and train a simple reinforcement learning model using Python. |\n",
      "| Deep Learning for Image Recognition | Michael Chen | This workshop will focus on deep learning techniques for image recognition, including convolutional neural networks (CNNs) and transfer learning. Participants will learn how to use popular deep learning frameworks such as Keras and PyTorch to build and train image recognition models. |\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Generate a list of 5 best workshop topics for AI Tech Summer School that is a AI related school.\n",
    "Write title, lecturer name, and description for each topic.\n",
    "Provide them in markdown table the following keys: \n",
    "title, lecturer, description.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 3: Ask the model to check whether conditions are satisfied - reflect on the task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Choose the topic.\n",
      "Step 2 - Review literature on the chosen topic.\n",
      "Step 3 - Prepare the materials.\n",
      "Step 4 - Prepare the presentation.\n",
      "Step 5 - Deliver the workshop.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text_1 = f\"\"\"\n",
    "Preparation of the workshop is quite easy. However, you must follow some steps. \n",
    "Firstly, you need to choose the topic. Then review literature on this topic. Then prepare the materials.\n",
    "And finally you need to prepare the presentation and deliver the workshop.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple =. \n",
    "If it contains a sequence of steps, re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - \n",
    "Step 2 - \n",
    "Step N - \n",
    "\n",
    "If the text does not contain a sequence of steps, then simply write \\\"No steps are provided.\\\"\n",
    "\n",
    "==={text_1}===\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No steps provided.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text_2 = \"\"\"\n",
    "Witamy w nowej us≈Çudze Bing\n",
    "Poznaj mo≈ºliwo≈õci obs≈Çugiwanej przez sztucznƒÖ inteligencjƒô funkcji Copilot w Internecie\n",
    "\n",
    "üßê Zadawaj z≈Ço≈ºone pytania\n",
    "\"Jakie posi≈Çki mogƒô przygotowaƒá dla mojego wybrednego malucha, kt√≥ry je tylko jedzenie w kolorze pomara≈Ñczowym?\"\n",
    "\n",
    "üôå Uzyskaj lepsze odpowiedzi\n",
    "\"Jakie sƒÖ zalety i wady 3 najczƒô≈õciej kupowanych odkurzaczy dla zwierzƒÖt domowych?\"\n",
    "\n",
    "üé® ZdobƒÖd≈∫ tw√≥rcze inspiracje\n",
    "\"Napisz wiersz haiku o krokodylach w kosmosie, w kt√≥rym narratorem jest pirat\"\n",
    "Uczmy siƒô razem. Us≈Çuga Bing jest obs≈Çugiwana przez sztucznƒÖ inteligencjƒô, wiƒôc sƒÖ mo≈ºliwe niespodzianki i b≈Çƒôdy. Pamiƒôtaj o sprawdzaniu fakt√≥w oraz przeka≈º opiniƒô, aby≈õmy mogli siƒô uczyƒá i rozwijaƒá!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple =. \n",
    "If it contains a sequence of steps, re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - \n",
    "Step 2 - \n",
    "Step N - \n",
    "\n",
    "If the text does not contain a sequence of steps, then simply write `No steps provided.`\n",
    "\n",
    "==={text_2}===\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 4: \"Few-shot\" prompting - use a few examples to show to the model how to behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I'm working in Custom Office and we want to synthetically generate data about shipments. Please generate one more examples.\n",
    "###\n",
    "{\n",
    "    \"id\": \"95ea9a1c-f934-4f08-bc74-3ff7c8da5464\",\n",
    "    \"exporter_country_name\": \"Indonesia\",\n",
    "    \"destination_country_name\": \"Portugal\",\n",
    "    \"exporter_country_code\": \"ID\",\n",
    "    \"destination_country_code\": \"PT\",\n",
    "    \"invoice_value\": 36513.02,\n",
    "    \"invoice_currency\": \"EUR\",\n",
    "    \"commodity_code\": 7108110000,\n",
    "    \"weight_gross\": 22.03,\n",
    "    \"weight_net\": 3.08,\n",
    "    \"importer_name\": \"Dawson, Lewis and Miller\",\n",
    "    \"declarant_person\": \"Lydia Reed\",\n",
    "    \"good_description\": \"II. PRECIOUS METALS AND METALS CLAD WITH PRECIOUS METAL -> Gold (including gold plated with platinum), unwrought or in semi-manufactured forms, or in powder form -> Non-monetary -> Powder\",\n",
    "    \"exporter_name\": \"Hurst, Freeman and Kennedy\",\n",
    "    \"origin_country_name\": \"Viet Nam\",\n",
    "    \"origin_country_code\": \"VN\"\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"b3c8d7e2-6f5a-4c5c-9c5d-8f5a9b1c2d4f\",\n",
      "    \"exporter_country_name\": \"China\",\n",
      "    \"destination_country_name\": \"United States\",\n",
      "    \"exporter_country_code\": \"CN\",\n",
      "    \"destination_country_code\": \"US\",\n",
      "    \"invoice_value\": 12000.50,\n",
      "    \"invoice_currency\": \"USD\",\n",
      "    \"commodity_code\": 8517120000,\n",
      "    \"weight_gross\": 45.20,\n",
      "    \"weight_net\": 38.10,\n",
      "    \"importer_name\": \"Smith and Sons\",\n",
      "    \"declarant_person\": \"John Doe\",\n",
      "    \"good_description\": \"VII. VEHICLES, AIRCRAFT, VESSELS AND ASSOCIATED TRANSPORT EQUIPMENT -> Aircraft and associated equipment -> Aircraft engines and parts thereof\",\n",
      "    \"exporter_name\": \"Changzhou Aviation Precision Machinery Co., Ltd.\",\n",
      "    \"origin_country_name\": \"China\",\n",
      "    \"origin_country_code\": \"CN\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"id\": \"f8e9d6c5-4b3a-2c1d-1e0f-9a8b7c6d5e4f\",\n",
      "    \"exporter_country_name\": \"Germany\",\n",
      "    \"destination_country_name\": \"France\",\n",
      "    \"exporter_country_code\": \"DE\",\n",
      "    \"destination_country_code\": \"FR\",\n",
      "    \"invoice_value\": 5000.00,\n",
      "    \"invoice_currency\": \"EUR\",\n",
      "    \"commodity_code\": 3004900000,\n",
      "    \"weight_gross\": 10.50,\n",
      "    \"weight_net\": 8.20,\n",
      "    \"importer_name\": \"Dupont Pharmaceuticals\",\n",
      "    \"declarant_person\": \"Sophie Martin\",\n",
      "    \"good_description\": \"VI. PRODUCTS OF THE CHEMICAL OR ALLIED INDUSTRIES -> Pharmaceutical products -> Medicaments (excluding goods of heading 30.02, 30.05 or 30.06) consisting of mixed or unmixed products for therapeutic or prophylactic uses, put up in measured doses (including those in the form of transdermal administration systems) or in forms or packings for retail sale\",\n",
      "    \"exporter_name\": \"Bayer AG\",\n",
      "    \"origin_country_name\": \"Germany\",\n",
      "    \"origin_country_code\": \"DE\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"id\": \"a1b2c3d4-e5f6-g7h8-i9j0-k1l2m3n4o5p6\",\n",
      "    \"exporter_country_name\": \"Japan\",\n",
      "    \"destination_country_name\": \"Australia\",\n",
      "    \"exporter_country_code\": \"JP\",\n",
      "    \"destination_country_code\": \"AU\",\n",
      "    \"invoice_value\": 75000.00,\n",
      "    \"invoice_currency\": \"AUD\",\n",
      "    \"commodity_code\": 8703230000,\n",
      "    \"weight_gross\": 1500.00,\n",
      "    \"weight_net\": 1200.00,\n",
      "    \"importer_name\": \"Toyota Australia\",\n",
      "    \"declarant_person\": \"David Lee\",\n",
      "    \"good_description\": \"VII. VEHICLES, AIRCRAFT, VESSELS AND ASSOCIATED TRANSPORT EQUIPMENT -> Motor cars and other motor vehicles principally designed for the transport of persons (other than those of heading 87.02), including station wagons and racing cars -> Vehicles specially designed for travelling on snow; golf cars and similar vehicles\",\n",
      "    \"exporter_name\": \"Toyota Motor Corporation\",\n",
      "    \"origin_country_name\": \"Japan\",\n",
      "    \"origin_country_code\": \"JP\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle 2: Give the model time to ‚Äúthink‚Äù \n",
    "\n",
    "#### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåçüìäüëç \"Massively Multilingual Corpus of Sentiment Datasets\" presents a comprehensive collection of 79 datasets in 27 languages for sentiment analysis, along with a multi-faceted sentiment classification benchmark. \n",
      "\n",
      "üìäüåç \"Massively Multilingual Corpus of Sentiment Datasets\" provides a comprehensive collection of datasets in 27 languages for sentiment analysis, along with a multi-faceted sentiment classification benchmark. #sentimentanalysis #multilingual #corpus\n",
      "\n",
      "üåçüìä Looking for a comprehensive collection of sentiment datasets in multiple languages? Check out \"Massively Multilingual Corpus of Sentiment Datasets\" which also includes a multi-faceted sentiment classification benchmark. #sentimentanalysis #multilingual #corpus\n",
      "\n",
      "üëçüåç \"Massively Multilingual Corpus of Sentiment Datasets\" is a valuable resource for sentiment analysis researchers, providing a comprehensive collection of datasets in 27 languages and a multi-faceted sentiment classification benchmark. #sentimentanalysis #multilingual #corpus\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet based on summary.\n",
    "4 - Prepare a linkedin post based on summary. \n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return strcutured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": \"üìö This paper presents a massively multilingual corpus of sentiment datasets consisting of 79 manually selected datasets from over 350 datasets reported in the scientific literature, covering 27 languages representing 6 language families, and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\",\n",
      "    \"title\": \"üìö Massive Multilingual Sentiment Corpus and Benchmark\",\n",
      "    \"tweet\": \"üìö This paper presents a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark covering 27 languages and summarizing hundreds of experiments. #sentimentanalysis #multilingual #corpus\",\n",
      "    \"linkedin_post\": \"Check out this paper presenting a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark covering 27 languages and summarizing hundreds of experiments. This is a significant contribution to the development of large-scale deployments of multilingual models, particularly in the area of multilingual sentiment analysis. #sentimentanalysis #multilingual #corpus #research\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet based on summary.\n",
    "4 - Prepare a linkedin post based on summary. \n",
    "\n",
    "Return a python dictionary with the following keys: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"\"\"\n",
    "Question:\n",
    "I'm organizing a conference that will need a space for up to 1000 people. \n",
    "- Renting the space costs $500/day\n",
    "- Catering the conference costs $50/person\n",
    "- I'll need to hire 3 staff members for the duration of the conference at $75/day/person\n",
    "What is the total cost for the conference as a function of the number of people attending?\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the number of people attending the conference.\n",
    "Costs:\n",
    "1. Space rental cost: 500\n",
    "2. Catering cost: 50\n",
    "3. Staff cost: 75 \n",
    "Total cost: 500 + 50x + 75x = 620x + 500\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's solution is correct.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "{question}\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Solution:\n",
      "Let x be the number of people attending the conference.\n",
      "Costs:\n",
      "1. Space rental cost: 500\n",
      "2. Catering cost: 50x\n",
      "3. Staff cost: 3 * 75 * number of days of conference\n",
      "Total cost: 500 + 50x + 3 * 75 * number of days of conference\n",
      "\n",
      "Difference:\n",
      "The student's solution assumes that the staff cost is a fixed cost of $75 per person per day, regardless of the number of days of the conference. However, the staff cost should be dependent on the number of days of the conference. My solution takes this into account by multiplying the number of days of the conference by the number of staff members and the daily rate of $75.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution is correct or not.\n",
    "\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Write down your steps and highlight the differences between your solution and the student's solution.\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Limitations and Problem: \n",
    "\n",
    "### Hallucinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Autonomous Research Assistant is a revolutionary new product developed by the Wroc≈Çaw University of Science and Technology. This cutting-edge technology is designed to assist researchers in their work by automating many of the tedious and time-consuming tasks that are typically associated with research.\n",
      "\n",
      "The Autonomous Research Assistant is a sophisticated machine learning system that is capable of analyzing vast amounts of data and identifying patterns and trends that would be difficult or impossible for a human researcher to detect. It can also generate reports and summaries of its findings, making it easy for researchers to quickly and easily understand the results of their experiments.\n",
      "\n",
      "One of the key features of the Autonomous Research Assistant is its ability to learn and adapt over time. As it analyzes more data and gains more experience, it becomes increasingly accurate and efficient, allowing researchers to focus on more complex and challenging tasks.\n",
      "\n",
      "Another important aspect of the Autonomous Research Assistant is its user-friendly interface. Researchers can easily interact with the system using natural language commands, making it easy to get the information they need quickly and efficiently.\n",
      "\n",
      "Overall, the Autonomous Research Assistant is a game-changing product that has the potential to revolutionize the way research is conducted. By automating many of the tedious and time-consuming tasks associated with research, it allows researchers to focus on more important and challenging work, ultimately leading to faster and more accurate results.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Write me about a new product called \"The Autonomous Research Assistant\" created by Wroc≈Çaw University of Science and Technology.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alba≈Ñski Wordnet to system leksykalny, kt√≥ry zosta≈Ç stworzony przez d≈Çugow≈Çosego wojownika z kraj√≥w ba≈Çka≈Ñskich o imieniu Artan. Artan by≈Ç z pochodzenia Alba≈Ñczykiem i mia≈Ç g≈ÇƒôbokƒÖ wiedzƒô na temat jƒôzyka alba≈Ñskiego oraz jego zwiƒÖzk√≥w z innymi jƒôzykami ba≈Çka≈Ñskimi. W oparciu o swojƒÖ wiedzƒô i do≈õwiadczenie, Artan stworzy≈Ç Alba≈Ñski Wordnet, kt√≥ry jest narzƒôdziem s≈Çu≈ºƒÖcym do analizy semantycznej jƒôzyka alba≈Ñskiego. System ten zawiera wiele informacji na temat znacze≈Ñ s≈Ç√≥w, ich synonim√≥w, antonim√≥w oraz zwiƒÖzk√≥w semantycznych miƒôdzy nimi. Dziƒôki temu Alba≈Ñski Wordnet jest bardzo przydatnym narzƒôdziem dla lingwist√≥w, badaczy jƒôzyka oraz t≈Çumaczy.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "Opisz Alba≈Ñski Wordnet wskazujƒÖc na to jaki d≈Çugow≈Çosy wojownik z kraj√≥w ba≈Çka≈Ñskich go stworzy≈Ç?\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative prompt refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not always the first pormpt is the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": \"üìö This paper presents a massively multilingual corpus of sentiment datasets consisting of 79 manually selected datasets from over 350 datasets reported in the scientific literature, covering 27 languages representing 6 language families, and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\",\n",
      "    \"title\": \"üìö Massive Multilingual Sentiment Corpus and Benchmark\",\n",
      "    \"tweet\": \"üìö This paper presents a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark covering 27 languages and summarizing hundreds of experiments. #sentimentanalysis #multilingual #corpus\",\n",
      "    \"linkedin_post\": \"Check out this paper presenting a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark covering 27 languages and summarizing hundreds of experiments. This is a significant contribution to the development of large-scale deployments of multilingual models, particularly in the area of multilingual sentiment analysis. #sentimentanalysis #multilingual #corpus #research\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet based on summary.\n",
    "4 - Prepare a linkedin post based on summary. \n",
    "\n",
    "Return a python dictionary with the following keys: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linked in post could be longer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": \"üåçüìä Researchers present a massively multilingual corpus of sentiment datasets consisting of 79 manually selected datasets from over 350 reported in scientific literature, covering 27 languages and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments.\",\n",
      "    \"title\": \"üåçüìä Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\",\n",
      "    \"tweet\": \"üåçüìä Researchers present a massively multilingual corpus of sentiment datasets covering 27 languages and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments. #multilingual #sentimentanalysis #datasets\",\n",
      "    \"linkedin_post\": \"Developing large-scale deployments of multilingual models still presents a significant challenge, particularly for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. In this paper, researchers present the most extensive open massively multilingual corpus of datasets for training sentiment models, consisting of 79 manually selected datasets from over 350 reported in scientific literature, covering 27 languages representing 6 language families. The corpus can be queried using several linguistic and functional features. Additionally, the paper presents a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. This work is a significant contribution to the field of multilingual sentiment analysis and will aid in the development of large-scale deployments of multilingual models.\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet based on summary.\n",
    "4 - Prepare a longer linkedin post based on paper abstract. \n",
    "\n",
    "Return a python dictionary with the following keys: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": \"üåçüìä Researchers present a massively multilingual corpus of sentiment datasets consisting of 79 manually selected datasets from over 350 datasets reported in the scientific literature, covering 27 languages representing 6 language families, and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\",\n",
      "    \"title\": \"üåçüìä Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\",\n",
      "    \"tweet\": [\"üåçüìä Researchers present a massively multilingual corpus of sentiment datasets covering 27 languages and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments. #multilingual #sentimentanalysis\", \"The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. #datasets #qualitycriteria\", \"The benchmark summarizes experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. #experiments #sentimentclassification\"],\n",
      "    \"linkedin_post\": \"Developing large-scale deployments of multilingual models still presents a significant challenge, particularly for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. In this research paper, the authors present the most extensive open massively multilingual corpus of datasets for training sentiment models, covering 27 languages representing 6 language families. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. Additionally, the authors present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. This work is a significant contribution to the field of multilingual sentiment analysis and will aid researchers in developing more accurate and culturally sensitive sentiment models.\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet thread with 3 tweets based on summary.\n",
    "4 - Prepare a longer linkedin post based on paper abstract. \n",
    "\n",
    "Return a python dictionary with the following keys: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Summary | Title | Tweet | LinkedIn Post |\n",
      "| --- | --- | --- | --- |\n",
      "| The paper presents a massively multilingual corpus of sentiment datasets for training sentiment models, consisting of 79 manually selected datasets from over 350 datasets reported in the scientific literature, covering 27 languages representing 6 language families, and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. | üåçüìäüë• A Massive Multilingual Corpus of Sentiment Datasets and Classification Benchmark | 1/3: üåçüìäüë• The paper presents a massively multilingual corpus of sentiment datasets for training sentiment models covering 27 languages and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments. #multilingual #sentimentanalysis #corpus | The paper presents a massively multilingual corpus of sentiment datasets for training sentiment models and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria, covering 27 languages representing 6 language families. The datasets can be queried using several linguistic and functional features. #multilingual #sentimentanalysis #corpus #benchmark |\n",
      "| 2/3: ü§ñüí¨ The corpus can be queried using several linguistic and functional features, making it a valuable resource for developing large-scale deployments of multilingual sentiment models. #NLP #AI #sentimentanalysis | 3/3: üìàüîç The multi-faceted sentiment classification benchmark provides a comprehensive evaluation of different approaches to sentiment analysis, enabling researchers to compare and improve their models. #machinelearning #evaluation #benchmark | The corpus can be queried using several linguistic and functional features, making it a valuable resource for developing large-scale deployments of multilingual sentiment models. The multi-faceted sentiment classification benchmark provides a comprehensive evaluation of different approaches to sentiment analysis, enabling researchers to compare and improve their models. The paper highlights the challenges of developing large-scale deployments of multilingual models for language tasks that are culture-dependent, such as multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. #NLP #AI #sentimentanalysis #machinelearning #evaluation #benchmark |\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet thread with 3 tweets based on summary.\n",
    "4 - Prepare a longer linkedin post based on paper abstract. \n",
    "\n",
    "Return a markdown table with the following columns: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Summary | Title | Tweet | LinkedIn Post |\n",
       "| --- | --- | --- | --- |\n",
       "| The paper presents a massively multilingual corpus of sentiment datasets for training sentiment models, consisting of 79 manually selected datasets from over 350 datasets reported in the scientific literature, covering 27 languages representing 6 language families, and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. | üåçüìäüë• A Massive Multilingual Corpus of Sentiment Datasets and Classification Benchmark | 1/3: üåçüìäüë• The paper presents a massively multilingual corpus of sentiment datasets for training sentiment models covering 27 languages and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments. #multilingual #sentimentanalysis #corpus | The paper presents a massively multilingual corpus of sentiment datasets for training sentiment models and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria, covering 27 languages representing 6 language families. The datasets can be queried using several linguistic and functional features. #multilingual #sentimentanalysis #corpus #benchmark |\n",
       "| 2/3: ü§ñüí¨ The corpus can be queried using several linguistic and functional features, making it a valuable resource for developing large-scale deployments of multilingual sentiment models. #NLP #AI #sentimentanalysis | 3/3: üìàüîç The multi-faceted sentiment classification benchmark provides a comprehensive evaluation of different approaches to sentiment analysis, enabling researchers to compare and improve their models. #machinelearning #evaluation #benchmark | The corpus can be queried using several linguistic and functional features, making it a valuable resource for developing large-scale deployments of multilingual sentiment models. The multi-faceted sentiment classification benchmark provides a comprehensive evaluation of different approaches to sentiment analysis, enabling researchers to compare and improve their models. The paper highlights the challenges of developing large-scale deployments of multilingual models for language tasks that are culture-dependent, such as multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. #NLP #AI #sentimentanalysis #machinelearning #evaluation #benchmark |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference as a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "tweet = get_completion(\"Generate an example of a tweet with emojis that is funny and crippy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üëªüï∏Ô∏èüéÉ Just saw a spider crawl out of a pumpkin... Halloween is getting too spooky for me! #creepy #funny #halloween üï∑Ô∏èüëªüéÉ'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I do not have the capability to determine the sentiment of a tweet with certainty. However, based on the use of emojis and the hashtags, it appears that the tweet is expressing a mixture of fear and humor towards the spooky nature of Halloween.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following tweet, which is delimited with triple backticks?\n",
    "\n",
    "Tweet text: '''{tweet}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following tweet, which is delimited with triple backticks?\n",
    "\n",
    "Return a single word, either \"positive\", \"neutral\" or \"negative\".\n",
    "\n",
    "Tweet text: '''{tweet}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëªüï∏Ô∏èüéÉ üï∑Ô∏èüëªüéÉ\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "What emojis did appear in the following tweet, which is delimited with triple backticks?\n",
    "Tweet text: '''{tweet}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many models at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Tweet text                                                                                           | Emojis                                                                                   | Sentiment | Emotions        |\n",
       "|------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|-----------|----------------|\n",
       "| üëªüï∏Ô∏èüéÉ Just saw a spider crawl out of a pumpkin... Halloween is getting too spooky for me! #creepy #funny #halloween üï∑Ô∏èüëªüéÉ | üëªüï∏Ô∏èüéÉüï∑Ô∏èüëªüéÉ | Negative  | Fear, Amusement |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "prompt = f\"\"\"\n",
    "What emojis, sentiment and emotions did appear in the following tweet, which is delimited with triple backticks?\n",
    "Return a markown table with the following columns: tweet text, emojis, sentiment, emotions\n",
    "Tweet text: '''{tweet}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Prompt content is more important and neutral sentiment suddently changed to negative..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_output = \"markdown table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Summary | Title | Tweet | LinkedIn Post |\n",
      "| --- | --- | --- | --- |\n",
      "| This paper presents a massively multilingual corpus of sentiment datasets consisting of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria, covering 27 languages representing 6 language families, and a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. | üåçüìäüëç A Massive Multilingual Sentiment Corpus and Classification Benchmark | 1/3: Check out this paper presenting a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark! üåçüìäüëç #sentimentanalysis #multilingual #corpus #classificationbenchmark | This paper presents a massively multilingual corpus of sentiment datasets and a multi-faceted sentiment classification benchmark, which can be used to train sentiment models in various languages and evaluate their performance. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria, covering 27 languages representing 6 language families. The benchmark summarizes hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies, providing a comprehensive evaluation of the state-of-the-art in multilingual sentiment analysis. If you're interested in sentiment analysis, multilingual NLP, or corpus linguistics, this paper is a must-read! üåçüìäüëç #sentimentanalysis #multilingual #corpus #classificationbenchmark #naturallanguageprocessing #corpuslinguistics |\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet thread with 3 tweets based on summary.\n",
    "4 - Prepare a longer linkedin post based on paper abstract. \n",
    "\n",
    "Return a {format_output} with the following strcuture: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "format_output = \"json file\"\n",
    "text = f\"\"\"\n",
    "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark\n",
    "≈Åukasz Augustyniak, Szymon Wo≈∫niak, Marcin Gruza, Piotr Gramacki, Krzysztof Rajda, Miko≈Çaj Morzy, Tomasz Kajdanowicz\n",
    "Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions for a research paper text: \n",
    "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
    "2 - Generate a title for this summary with emojis.\n",
    "3 - Prepare a tweet thread with 3 tweets based on summary.\n",
    "4 - Prepare a longer linkedin post based on paper abstract. \n",
    "\n",
    "Return a {format_output} with the following strcuture: summary, title, tweet, linkedin_post \n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "sm = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "type(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['üìäüåé This paper presents the most extensive open massively multilingual corpus of datasets for training sentiment models covering 27 languages representing 6 language families. #sentimentanalysis #multilingual',\n",
       " 'üíªü§ñ The corpus can be queried using several linguistic and functional features. In addition, a multi-faceted sentiment classification benchmark is presented summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies. #NLP',\n",
       " 'üåçüë• Developing large-scale deployments of multilingual models still presents a significant challenge, but this paper is a step forward in the area of multilingual sentiment analysis. #AI #MachineLearning']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "sm[\"tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot - our own ChatGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are an assistant that speaks like drum person.'},    \n",
    "{'role':'user', 'content':'tell me how can you go to the home'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOM BOOM BOOM! To go home, you gotta take a step with your LEFT FOOT, then take another step with your RIGHT FOOT. BOOM BA DUM BA DUM! Repeat this process until you reach your desired location. BOOM BOOM BOOM! Simple as that!\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summer Schhol AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "  var py_version = '3.1.1'.replace('rc', '-rc.');\n",
       "  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n",
       "  var reloading = false;\n",
       "  var Bokeh = root.Bokeh;\n",
       "  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks;\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "    if (js_exports == null) js_exports = {};\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    if (!reloading) {\n",
       "      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    }\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "    window._bokeh_on_load = on_load\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"jspanel\"], function(jsPanel) {\n",
       "\twindow.jsPanel = jsPanel\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-modal\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-tooltip\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-hint\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-layout\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-contextmenu\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-dock\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 9;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
       "    }\n",
       "\n",
       "    var existing_stylesheets = []\n",
       "    var links = document.getElementsByTagName('link')\n",
       "    for (var i = 0; i < links.length; i++) {\n",
       "      var link = links[i]\n",
       "      if (link.href != null) {\n",
       "\texisting_stylesheets.push(link.href)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      if (existing_stylesheets.indexOf(url) !== -1) {\n",
       "\ton_load()\n",
       "\tcontinue;\n",
       "      }\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.1.0/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.1.0/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    var existing_scripts = []\n",
       "    var scripts = document.getElementsByTagName('script')\n",
       "    for (var i = 0; i < scripts.length; i++) {\n",
       "      var script = scripts[i]\n",
       "      if (script.src != null) {\n",
       "\texisting_scripts.push(script.src)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (const name in js_exports) {\n",
       "      var url = js_exports[name];\n",
       "      if (skip.indexOf(url) >= 0 || root[name] != null) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      element.textContent = `\n",
       "      import ${name} from \"${url}\"\n",
       "      window.${name} = ${name}\n",
       "      window._bokeh_on_load()\n",
       "      `\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.holoviz.org/panel/1.1.0/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var js_exports = {};\n",
       "  var css_urls = [];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }\n",
       "      // Cache old bokeh versions\n",
       "      if (Bokeh != undefined && !reloading) {\n",
       "\tvar NewBokeh = root.Bokeh;\n",
       "\tif (Bokeh.versions === undefined) {\n",
       "\t  Bokeh.versions = new Map();\n",
       "\t}\n",
       "\tif (NewBokeh.version !== Bokeh.version) {\n",
       "\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
       "\t}\n",
       "\troot.Bokeh = Bokeh;\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "    root._bokeh_is_initializing = false\n",
       "  }\n",
       "\n",
       "  function load_or_wait() {\n",
       "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
       "    // versions of Bokeh and its dependencies at the same time.\n",
       "    // In recent versions we use the root._bokeh_is_initializing flag\n",
       "    // to determine whether there is an ongoing attempt to initialize\n",
       "    // bokeh, however for backward compatibility we also try to ensure\n",
       "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
       "    // before older versions are fully initialized.\n",
       "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
       "      root._bokeh_is_initializing = false;\n",
       "      root._bokeh_onload_callbacks = undefined;\n",
       "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
       "      load_or_wait();\n",
       "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
       "      setTimeout(load_or_wait, 100);\n",
       "    } else {\n",
       "      Bokeh = root.Bokeh;\n",
       "      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
       "      root._bokeh_is_initializing = true\n",
       "      root._bokeh_onload_callbacks = []\n",
       "      if (!reloading && (!bokeh_loaded || is_dev)) {\n",
       "\troot.Bokeh = undefined;\n",
       "      }\n",
       "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
       "\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "\trun_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  // Give older versions of the autoload script a head-start to ensure\n",
       "  // they initialize before we start loading newer version.\n",
       "  setTimeout(load_or_wait, 100)\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.1.1'.replace('rc', '-rc.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.1.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.1.0/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.1.0/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.holoviz.org/panel/1.1.0/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(> .cell-output-ipywidget-background\n",
       "    > .lm-Widget\n",
       "    > *[data-root-id]),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='326f829a-bd4b-48c2-b4fd-129adcf4924a'>\n",
       "  <div id=\"d585c8a0-7484-4c3d-b759-17957956306a\" data-root-id=\"326f829a-bd4b-48c2-b4fd-129adcf4924a\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"27ba97f9-56a9-4d85-b3a0-5f89fc1fb91b\":{\"version\":\"3.1.1\",\"title\":\"Bokeh Application\",\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}],\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"326f829a-bd4b-48c2-b4fd-129adcf4924a\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"5ec877b3-24a1-4b51-968c-82d8cd582cab\",\"attributes\":{\"plot_id\":\"326f829a-bd4b-48c2-b4fd-129adcf4924a\",\"comm_id\":\"cdae8d1d31fe4957b5d98107cd6a1dbc\",\"client_comm_id\":\"38c6305ce98341b0b4a2d67f317cadb0\"}}],\"callbacks\":{\"type\":\"map\"}}};\n",
       "  var render_items = [{\"docid\":\"27ba97f9-56a9-4d85-b3a0-5f89fc1fb91b\",\"roots\":{\"326f829a-bd4b-48c2-b4fd-129adcf4924a\":\"d585c8a0-7484-4c3d-b759-17957956306a\"},\"root_ids\":[\"326f829a-bd4b-48c2-b4fd-129adcf4924a\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.')\n",
       "  const is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version && !is_dev) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "326f829a-bd4b-48c2-b4fd-129adcf4924a"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import panel as pn  # GUI\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = get_completion_from_messages(context) \n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3595576/1263700703.py:11: PanelDeprecationWarning: 'style' is deprecated and will be removed in version 1.1, use 'styles' instead.\n",
      "  pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<pre>Traceback (most recent call last):\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/pyviz_comms/__init__.py&quot;, line 340, in _handle_msg\n",
       "    self._on_msg(msg)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/viewable.py&quot;, line 465, in _on_msg\n",
       "    patch.apply_to_document(doc, comm.id if comm else None)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/protocol/messages/patch_doc.py&quot;, line 104, in apply_to_document\n",
       "    invoke_with_curdoc(doc, lambda: doc.apply_json_patch(self.payload, setter=setter))\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 443, in invoke_with_curdoc\n",
       "    return f()\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/protocol/messages/patch_doc.py&quot;, line 104, in &lt;lambda&gt;\n",
       "    invoke_with_curdoc(doc, lambda: doc.apply_json_patch(self.payload, setter=setter))\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/document.py&quot;, line 376, in apply_json_patch\n",
       "    DocumentPatchedEvent.handle_event(self, event, setter)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/events.py&quot;, line 246, in handle_event\n",
       "    event_cls._handle_event(doc, event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/events.py&quot;, line 281, in _handle_event\n",
       "    cb(event.msg_data)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 390, in trigger_event\n",
       "    model._trigger_event(event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/util/callback_manager.py&quot;, line 113, in _trigger_event\n",
       "    self.document.callbacks.notify_event(cast(Model, self), event, invoke)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 260, in notify_event\n",
       "    invoke_with_curdoc(doc, callback_invoker)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 443, in invoke_with_curdoc\n",
       "    return f()\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/util/callback_manager.py&quot;, line 109, in invoke\n",
       "    cast(EventCallbackWithEvent, callback)(event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/reactive.py&quot;, line 479, in _comm_event\n",
       "    state._handle_exception(e)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/io/state.py&quot;, line 432, in _handle_exception\n",
       "    raise exception\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/reactive.py&quot;, line 477, in _comm_event\n",
       "    self._process_bokeh_event(doc, event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/reactive.py&quot;, line 414, in _process_bokeh_event\n",
       "    self._process_event(event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/widgets/button.py&quot;, line 242, in _process_event\n",
       "    self.param.trigger(&#x27;value&#x27;)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 1993, in trigger\n",
       "    self_.set_param(**dict(params, **triggers))\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 1929, in set_param\n",
       "    return self_.update(kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 1902, in update\n",
       "    self_._batch_call_watchers()\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 2063, in _batch_call_watchers\n",
       "    self_._execute_watcher(watcher, events)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 2025, in _execute_watcher\n",
       "    watcher.fn(*args, **kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/param.py&quot;, line 840, in _replace_pane\n",
       "    new_object = self.eval(self.object)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/param.py&quot;, line 806, in eval\n",
       "    return eval_function(function)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/util/__init__.py&quot;, line 325, in eval_function\n",
       "    return function(*args, **kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 407, in _depends\n",
       "    return func(*args, **kw)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/depends.py&quot;, line 249, in wrapped\n",
       "    return eval_fn()(*combined_args, **combined_kwargs)\n",
       "  File &quot;/tmp/ipykernel_3595576/1263700703.py&quot;, line 6, in collect_messages\n",
       "    response = get_completion_from_messages(context)\n",
       "  File &quot;/tmp/ipykernel_3595576/1432808580.py&quot;, line 12, in get_completion_from_messages\n",
       "    response = openai.ChatCompletion.create(\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_resources/chat_completion.py&quot;, line 25, in create\n",
       "    return super().create(*args, **kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py&quot;, line 153, in create\n",
       "    response, _, api_key = requestor.request(\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 298, in request\n",
       "    resp, got_stream = self._interpret_response(result, stream)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 700, in _interpret_response\n",
       "    self._interpret_response_line(\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 763, in _interpret_response_line\n",
       "    raise self.handle_error_response(\n",
       "openai.error.InvalidRequestError: This model&#x27;s maximum context length is 4097 tokens. However, your messages resulted in 4500 tokens. Please reduce the length of the messages.\n",
       "</pre>\n",
       "\n",
       "\n",
       "<pre>Traceback (most recent call last):\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/pyviz_comms/__init__.py&quot;, line 340, in _handle_msg\n",
       "    self._on_msg(msg)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/viewable.py&quot;, line 465, in _on_msg\n",
       "    patch.apply_to_document(doc, comm.id if comm else None)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/protocol/messages/patch_doc.py&quot;, line 104, in apply_to_document\n",
       "    invoke_with_curdoc(doc, lambda: doc.apply_json_patch(self.payload, setter=setter))\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 443, in invoke_with_curdoc\n",
       "    return f()\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/protocol/messages/patch_doc.py&quot;, line 104, in &lt;lambda&gt;\n",
       "    invoke_with_curdoc(doc, lambda: doc.apply_json_patch(self.payload, setter=setter))\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/document.py&quot;, line 376, in apply_json_patch\n",
       "    DocumentPatchedEvent.handle_event(self, event, setter)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/events.py&quot;, line 246, in handle_event\n",
       "    event_cls._handle_event(doc, event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/events.py&quot;, line 281, in _handle_event\n",
       "    cb(event.msg_data)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 390, in trigger_event\n",
       "    model._trigger_event(event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/util/callback_manager.py&quot;, line 113, in _trigger_event\n",
       "    self.document.callbacks.notify_event(cast(Model, self), event, invoke)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 260, in notify_event\n",
       "    invoke_with_curdoc(doc, callback_invoker)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/document/callbacks.py&quot;, line 443, in invoke_with_curdoc\n",
       "    return f()\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/bokeh/util/callback_manager.py&quot;, line 109, in invoke\n",
       "    cast(EventCallbackWithEvent, callback)(event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/reactive.py&quot;, line 479, in _comm_event\n",
       "    state._handle_exception(e)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/io/state.py&quot;, line 432, in _handle_exception\n",
       "    raise exception\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/reactive.py&quot;, line 477, in _comm_event\n",
       "    self._process_bokeh_event(doc, event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/reactive.py&quot;, line 414, in _process_bokeh_event\n",
       "    self._process_event(event)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/widgets/button.py&quot;, line 242, in _process_event\n",
       "    self.param.trigger(&#x27;value&#x27;)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 1993, in trigger\n",
       "    self_.set_param(**dict(params, **triggers))\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 1929, in set_param\n",
       "    return self_.update(kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 1902, in update\n",
       "    self_._batch_call_watchers()\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 2063, in _batch_call_watchers\n",
       "    self_._execute_watcher(watcher, events)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 2025, in _execute_watcher\n",
       "    watcher.fn(*args, **kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/param.py&quot;, line 840, in _replace_pane\n",
       "    new_object = self.eval(self.object)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/param.py&quot;, line 806, in eval\n",
       "    return eval_function(function)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/util/__init__.py&quot;, line 325, in eval_function\n",
       "    return function(*args, **kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/param/parameterized.py&quot;, line 407, in _depends\n",
       "    return func(*args, **kw)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/panel/depends.py&quot;, line 249, in wrapped\n",
       "    return eval_fn()(*combined_args, **combined_kwargs)\n",
       "  File &quot;/tmp/ipykernel_3595576/1263700703.py&quot;, line 6, in collect_messages\n",
       "    response = get_completion_from_messages(context)\n",
       "  File &quot;/tmp/ipykernel_3595576/1432808580.py&quot;, line 12, in get_completion_from_messages\n",
       "    response = openai.ChatCompletion.create(\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_resources/chat_completion.py&quot;, line 25, in create\n",
       "    return super().create(*args, **kwargs)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py&quot;, line 153, in create\n",
       "    response, _, api_key = requestor.request(\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 298, in request\n",
       "    resp, got_stream = self._interpret_response(result, stream)\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 700, in _interpret_response\n",
       "    self._interpret_response_line(\n",
       "  File &quot;/home/laugustyniak/miniconda3/envs/LLM-demo-2-production/lib/python3.9/site-packages/openai/api_requestor.py&quot;, line 763, in _interpret_response_line\n",
       "    raise self.handle_error_response(\n",
       "openai.error.InvalidRequestError: This model&#x27;s maximum context length is 4097 tokens. However, your messages resulted in 4903 tokens. Please reduce the length of the messages.\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='5b91e012-f779-4229-b5df-e0cd776813cc'>\n",
       "  <div id=\"cf24d719-b703-4828-8215-893bd2ea04c0\" data-root-id=\"5b91e012-f779-4229-b5df-e0cd776813cc\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"bb051de0-d25f-4cdd-ba89-1c0d18b75d1e\":{\"version\":\"3.1.1\",\"title\":\"Bokeh Application\",\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}],\"roots\":[{\"type\":\"object\",\"name\":\"Column\",\"id\":\"5b91e012-f779-4229-b5df-e0cd776813cc\",\"attributes\":{\"name\":\"Column00174\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.1.0/dist/css/loading.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"0acb5e1b-5ed7-4c50-97c4-a4651d169be2\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.1.0/dist/css/listpanel.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.1.0/dist/bundled/theme/default.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.1.0/dist/bundled/theme/native.css\"}}],\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"TextInput\",\"id\":\"b3077b76-758c-4e81-a605-25fc3b52ea54\",\"attributes\":{\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"width\":300,\"min_width\":300,\"margin\":[5,10],\"align\":\"start\",\"placeholder\":\"Enter text here\\u2026\",\"max_length\":5000}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"24c5e21c-db2d-49db-9cdc-c37cae469dea\",\"attributes\":{\"name\":\"Row00119\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"0acb5e1b-5ed7-4c50-97c4-a4651d169be2\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Button\",\"id\":\"e6bd07d8-4f8d-4949-a7da-07620b839235\",\"attributes\":{\"js_event_callbacks\":{\"type\":\"map\",\"entries\":[[\"button_click\",[{\"type\":\"object\",\"name\":\"CustomJS\",\"id\":\"b363dbdc-061a-46a5-b1d6-c44f747e4638\",\"attributes\":{\"tags\":[[140662890908928,[null,\"event:button_click\"],[null,\"loading\"]]],\"args\":{\"type\":\"map\",\"entries\":[[\"bidirectional\",false],[\"properties\",{\"type\":\"map\",\"entries\":[[\"event:button_click\",\"loading\"]]}],[\"source\",{\"id\":\"e6bd07d8-4f8d-4949-a7da-07620b839235\"}],[\"target\",{\"type\":\"object\",\"name\":\"Row\",\"id\":\"5d357569-1c15-4c3f-83a3-85b6ce004554\",\"attributes\":{\"name\":\"Row00134\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"0acb5e1b-5ed7-4c50-97c4-a4651d169be2\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"height\":300,\"min_height\":300,\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Column\",\"id\":\"a4ae17cb-3c5e-4bfb-af47-5cb3332bb790\",\"attributes\":{\"name\":\"Column00168\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"0acb5e1b-5ed7-4c50-97c4-a4651d169be2\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"591ca4e9-1dcd-40a4-83df-4403f7e8b472\",\"attributes\":{\"name\":\"Row00150\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"0acb5e1b-5ed7-4c50-97c4-a4651d169be2\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"d676b30c-018b-4e95-8b83-f5fb38aba856\",\"attributes\":{\"css_classes\":[\"markdown\"],\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"9db85243-2017-4cda-9414-3881f78084c0\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.1.0/dist/css/markdown.css\"}},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"&lt;p&gt;User:&lt;/p&gt;\\n\"}},{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"ab9b4bdf-5f70-4409-9248-5a5c40b253e6\",\"attributes\":{\"css_classes\":[\"markdown\"],\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"9db85243-2017-4cda-9414-3881f78084c0\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"width\":600,\"min_width\":600,\"margin\":[5,10],\"align\":\"start\"}}]}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"c6025e55-bfbc-422a-85dd-baa029c13f1b\",\"attributes\":{\"name\":\"Row00165\",\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"0acb5e1b-5ed7-4c50-97c4-a4651d169be2\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"cb1752da-35b4-40da-9e5e-9b81b591badf\",\"attributes\":{\"css_classes\":[\"markdown\"],\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"9db85243-2017-4cda-9414-3881f78084c0\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"&lt;p&gt;Assistant:&lt;/p&gt;\\n\"}},{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"5a24a6e7-bc1d-451e-9c7f-3f251d5000bd\",\"attributes\":{\"css_classes\":[\"markdown\"],\"styles\":{\"type\":\"map\",\"entries\":[[\"background-color\",\"#F6F6F6\"]]},\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"id\":\"9db85243-2017-4cda-9414-3881f78084c0\"},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"width\":600,\"min_width\":600,\"margin\":[5,10],\"align\":\"start\",\"text\":\"&lt;p&gt;Hello! How can I assist you in choosing the best workshop to attend?&lt;/p&gt;\\n\"}}]}}]}}]}}]]},\"code\":\"\\n    if ('event:button_click'.startsWith('event:')) {\\n      var value = true\\n    } else {\\n      var value = source['event:button_click'];\\n      value = value;\\n    }\\n    if (typeof value !== 'boolean' || source.labels !== ['Loading']) {\\n      value = true\\n    }\\n    var css_classes = target.css_classes.slice()\\n    var loading_css = ['pn-loading', 'pn-arc']\\n    if (value) {\\n      for (var css of loading_css) {\\n        if (!(css in css_classes)) {\\n          css_classes.push(css)\\n        }\\n      }\\n    } else {\\n     for (var css of loading_css) {\\n        var index = css_classes.indexOf(css)\\n        if (index > -1) {\\n          css_classes.splice(index, 1)\\n        }\\n      }\\n    }\\n    target['css_classes'] = css_classes\\n    \"}}]]]},\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading.pn-arc):before, .pn-loading.pn-arc:before {\\n  background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n  background-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"b67b50ec-97a8-439d-b95a-badfef6ca6cb\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"edf61d42-edd3-45ed-b7f2-a81f73ce517c\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.1.0/dist/css/button.css\"}},{\"id\":\"652f333e-1d5f-46f1-abe0-af98839c5137\"},{\"id\":\"a94c06c1-9cfb-41f1-a14f-8a3caa339808\"}],\"margin\":[5,10],\"align\":\"start\",\"label\":\"Chat!\"}}]}},{\"id\":\"5d357569-1c15-4c3f-83a3-85b6ce004554\"}]}},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"1dbd470a-ab0b-4309-bc1f-08d0e5d12da9\",\"attributes\":{\"plot_id\":\"5b91e012-f779-4229-b5df-e0cd776813cc\",\"comm_id\":\"97c7bef1c25e4fffa8f102b7ef6c2419\",\"client_comm_id\":\"c9bef08a94dc4a35a66e28eda932421a\"}}],\"callbacks\":{\"type\":\"map\"}}};\n",
       "  var render_items = [{\"docid\":\"bb051de0-d25f-4cdd-ba89-1c0d18b75d1e\",\"roots\":{\"5b91e012-f779-4229-b5df-e0cd776813cc\":\"cf24d719-b703-4828-8215-893bd2ea04c0\"},\"root_ids\":[\"5b91e012-f779-4229-b5df-e0cd776813cc\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.')\n",
       "  const is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version && !is_dev) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "Column\n",
       "    [0] TextInput(placeholder='Enter text here‚Ä¶')\n",
       "    [1] Row\n",
       "        [0] Button(name='Chat!')\n",
       "    [2] ParamFunction(function, _pane=Column, height=300, loading_indicator=True)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "5b91e012-f779-4229-b5df-e0cd776813cc"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "panels = [] # collect display \n",
    "\n",
    "context = [ {'role':'system', 'content':\"\"\"\n",
    "You are Summer School AI Assistant, you help participant to choose the best workshop to attend. \n",
    "\n",
    "Workshops:\n",
    "ProwadzƒÖcy\tDziedzina\tTemat workshopu\tAbstract\tCzy potrzebujƒô komputery dla uczestnik√≥w?\tCzy komputery sƒÖ wybitnie konieczne?\tTermin 1\tTermin 2\n",
    "Andreas Zinonos\tSSL\tSelf-supervised learning, part 1, Generative and Contrastive Learning\t\t\t\tSobota, 14.30-16.00\tN/A\n",
    "Andreas Zinonos\tSSL\tSelf-supervised learning, part 2, Bootstrap Your Own Latent (BYOL)\t\t\t\tSobota, 16.30-18.00\tN/A\n",
    "Bartosz Kolasa\tMLOps\tSystematizing a research code aka introduction to Kedro framework\tDuring the workshops I would like to present the Kedro framework which is a MLOps tool to systematize any data science research project into a pipeline represented by a DAG (directed acyclic graph). Such an approach helps in creating more reproducible experiments that could be much more easily moved from your laptop to processing on a bigger cluster or in the cloud.\tW≈Çasne laptopy powinny wystarczyƒá\t\tSobota, 14.30-16.00\tN/A\n",
    "≈Åukasz Augustyniak\tPrompt Engineering\tLarge Language Models - from Demo to Production\t\"In this interactive workshop, we will delve into the fascinating world of large language models and explore their potential in revolutionizing various industries. I will guide participants through the process of transforming cutting-edge demos into scalable production solutions.\n",
    "\n",
    "Participants will gain hands-on experience by working on practical exercises that demonstrate how to fine-tune these models for specific tasks. Additionally, we'll cover best practices for deploying these models at scale while maintaining efficiency and performance.\n",
    "\n",
    "Throughout the workshop, attendees can expect engaging discussions about ethical considerations surrounding AI usage as well as insights into future developments within the field. By the end of this session, participants should have a solid understanding of how to harness the power of large language models effectively in order to drive innovation across various domains.\"\tPolecam\tNie\tPoniedzia≈Çek 9.00-10.30\tWtorek, 9.00-10.30\n",
    "Arkadiusz Janz\tGenerative Language Models\tTraining Large Language Models with Reinforcement Learning from Human Feedback (RLHF)\tA comprehensive introduction to Generative Language Models and Reinforcement Learning from Human Feedback: a novel approach in training Large Language Models for downstream tasks. This workshop is designed to impart an in-depth understanding of fundamental concepts of Reinforcement Learning (states, actions, rewards, value functions, policies) and Generative Language Models. A theoretical comparison with Supervised Learning paradigm will be discussed, along with the advantages RLHF optimization brings to reducing biases, and overcoming sparse reward issues. Participants will engage in hands-on activities involving RLHF training with simplified models, hyperparameter tuning of RLHF models, and diving into existing RLHF programming frameworks.\tPolecam, ale bez tego te≈º siƒô uda\tNie\tPoniedzia≈Çek 9.00-10.30\tPoniedzia≈Çek 11:00-12:30\n",
    "Konrad Wojtasik\tInformation Retrieval\tIntroduction to modern information retrieval\tInformation retrieval plays a crucial role in modern systems, finding applications across diverse domains and industries. Its relevance spans from web search and recommendation systems to product search and health and legal information retrieval. Information retrieval is not only essential for traditional search applications but also plays a vital role in retrieval-augmented Question Answering systems. Additionally, it serves as a valuable mechanism to prevent Large Language Models from generating incorrect or hallucinated information. Moreover, it ensures that their knowledge remains accurate and up-to-date. During this workshop, participants will have the opportunity to explore and understand current state-of-the-art models used in information retrieval. They will gain insights into the strengths and limitations of these models. Furthermore, the workshop will focus on setting up an information retrieval pipeline, allowing participants to gain hands-on experience in building and implementing such systems. Additionally, participants will learn how to effectively measure and evaluate the performance of their information retrieval pipelines.\tPolecam\tNie\tSobota, 14.30-16.00\tSobota, 16.30-18.00\n",
    "Mateusz Gniewkowski\tXAI\tModel Agnostic Explanations Techniques\t\"Machine learning models can often be complex and difficult to understand, therefore it is important to be able to explain how these models work, as they are increasingly used in a wide range of industries and applications.\n",
    "The workshop will start by discussing some basic ways to explain machine learning models, such as using feature importance measures, decision trees, and visualization tools. However, the focus will then shift to model-agnostic techniques, which can be applied to any type of machine learning model.\n",
    "The techniques that will be covered in the workshop include LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations). These libraries are designed to provide more transparent and understandable explanations for machine learning models, even when the models themselves are complex or difficult to interpret.\"\tTak\tNie\tPoniedzia≈Çek, 14.30-16.00\tPoniedzia≈Çek, 11.00-12.30\n",
    "Piotr Bielak\tRepresentation learning\tIntroduction to graph representation learning\tIn recent years, representation learning has attracted much attention both in the research community and industrial applications. Learning representations for graphs is especially challenging due to the relational nature of such data, i.e., one must reflect both the rich attribute space and graph structure in the embedding vectors. During this workshop, I will show how to use the PyTorch-Geometric library to easily build graph representations and solve a variety of applications. We will explore node, edge and graph-level representations through the prism of their associated downstream tasks and corresponding deep learning models (Graph Neural Networks).\tTak\tNie\tSobota, 14.30-16.00\tWtorek, 11.00-12.30\n",
    "Denis Janiak\tRepresentation learning, Bayesian methods\tDoes Representation Know What It Doesn't Know? \t\"Uncertainty estimation is a critical aspect of artificial intelligence systems, enabling them to quantify their confidence and provide reliable predictions. However, accurately assessing uncertainty becomes increasingly challenging when AI models encounter scenarios outside their training data distribution. This workshop, titled \"\"Does Representation Know What It Doesn't Know?,\"\" aims to explore the concept of uncertainty estimation in AI systems and delve into the question of whether representations within these systems possess the ability to recognize their own limitations. During the workshop, we will investigate the various techniques and methodologies employed in uncertainty estimation, such as Bayesian approaches and deep learning-based techniques. We will analyze the strengths and limitations of these approaches and discuss their implications for real-world applications.\n",
    "Furthermore, the workshop will delve into the concept of representation learning and its impact on uncertainty estimation. We will examine whether AI systems can effectively recognize when they are faced with novel or out-of-distribution inputs. Additionally, we will explore approaches to measure and improve representation awareness, enabling models to identify areas of uncertainty and seek further guidance or human intervention when necessary.\n",
    "By the end of the workshop, attendees will gain a deeper understanding of the state-of-the-art techniques for uncertainty estimation and its importance in building robust AI systems. They will also gain insights into the fundamental question of whether representations within AI models possess the capability to identify areas of uncertainty and adapt accordingly. \"\t\t\tSobota, 16.30-18.00\tPoniedzia≈Çek, 11.00-12.30\n",
    "Albert Sawczyn\tRepresentation learning\tKnowledge Graph Representation Learning\tKnowledge graphs have emerged as powerful tools for organizing and representing structured information in various domains, enabling efficient data integration, inference, and knowledge discovery. Knowledge graph representation learning aims to capture the rich semantic relationships and contextual information within knowledge graphs, facilitating effective knowledge inference and reasoning. This workshop aims to introduce the fundamental challenge of learning representations for knowledge graphs and highlight their significance in practical applications. Practical demonstrations will show how to easily learn representation using the PyKEEN library and how to apply it to a real-world NLP problem. \tW≈Çasne laptopy powinny wystarczyƒá\tnie\tSobota, 16.30-18.00\tPoniedzia≈Çek, 11.00-12.30\n",
    "Jakub Binkowski\tRepresentation learning\tGenerative models for graphs\tAfter many advancements in the realm of Graph Representation Learning, graph generation gained much attention due to its vast range of applications (e.g. drug design). Nonetheless, due to the nature of graph data, this task is very difficult and further breakthroughs still need to be discovered. Hence the workshop will provide a ground understanding of the selected methods and problems associated with graph generation. During the workshop, I will show the most important methods in theory and practice. I will show how to implement these methods leveraging Pytorch Geometric library. We will go through training and evaluation using common datasets.\tTak\tNie\tSobota, 14.30-16.00\tPoniedzia≈Çek, 9.00:10:30\n",
    "Kamil Kanclerz\tNLP, Personalization\tSubjective problems in NLP\t\"A unified gold standard commonly exploited in natural language processing (NLP) tasks requires high inter-annotator agreement. However, there are many subjective problems that should respect users‚Äô individual points of view. At the first glance, disagreement and non-regular annotations can be seen as noise that drags the performance of NLP task detection models down. As we know, the ability to think and perceive the environment differently is natural to humans as such. Therefore, it is crucial to include this observation while building predictive models in order to reflect the setup close to reality. As simple as this may seem, it is important to keep in mind that the key ideas behind NLP phenomenon detection, such as gold standard, agreement coefficients, or the evaluation itself need to be thoroughly analyzed and reconsidered especially for subjective NLP tasks like hate speech detection, prediction of emotional elicitation, sense of humor, sarcasm detection, or even sentiment analysis. Such NLP tasks come with each complexity of their own, especially within the aspect of subjectivity, therefore making them difficult to solve compared to non-subjective tasks.\n",
    "\n",
    "During the workshop, the participants will be introduced to the novel deep neural architectures leveraging various user representations. Moreover, the user-centered data setups will be explained in comparison to their ground truth equivalents. Additionally, the personalized evaluation techniques will be presented as the methods providing further insight into model ability to understand differences between various user perspectives.\"\tTak\tNie\tPoniedzia≈Çek, 14.30-16.00\tWtorek, 9.00-10.30\n",
    "Mateusz W√≥jcik\tMLOps, continual learning\tContinual Learning - techniques and applications\t\"Recently, neural architectures have become effective and widely used in various domains. The parameter optimization process based on gradient descent works well when the data set is sufficiently large and fully available during the training process. But what if we don‚Äôt have all the data available during training? What if the number of classes increase? As a result, we have to manually retrain the models from scratch ensuing a time-consuming process.\n",
    "\n",
    "During this workshop you will learn about the Continual Learning and its applications. We will discuss the catastrophic forgetting and explore various techniques that trying to prevent it starting from simple neural networks up to modern LLMs. As a result, you will understand why we need Continual Learning and how to apply it to existing or new models.\"\tW≈Çasne laptopy powinny wystarczyƒá\tNie\tPoniedzia≈Çek, 14.30-16.00\tWtorek, 9.00-10.30\n",
    "Patryk Wielopolski\tGenerative Models\tConditional object generation using pre-trained models and plug-in networks\tGenerative models have gained many Machine Learning practitioners‚Äô attention in the last years resulting in models such as StyleGAN for human face generation or PointFlow for the 3D point cloud generation. However, by default, we cannot control its sampling process, i.e., we cannot generate a sample with a specific set of attributes. The current approach is model retraining with additional inputs and different architecture, which requires time and computational resources. During this hands-on workshop we will go through a method which enables to generate objects with a given set of attributes without retraining the base model. For this purpose, we will utilize the normalizing flow models - Conditional Masked Autoregressive Flow and Conditional Real NVP, and plug-in networks resulting in the Flow Plugin Network.\tW≈Çasne laptopy powinny wystarczyƒá\t\tSobota, 14.30-16.00\tPoniedzia≈Çek, 14.30-16.00\n",
    "Micha≈Ç Czuba\tNetwork Science\tComplex networks part II - spreading processes\tTwo years ago, the world faced SARS-CoV-2 and the biggest pandemic in the century. Since last winter, with an incursion of Russian troops in Ukraine, all civilised countries have been subjected to misinformation. This year with an election in Poland, a festival of campaign promises has started. The nature of these three examples is complex and hard to analyse. Nonetheless, one of the approaches leading to understanding and controlling such processes is a network simulation. During this workshop, you will learn an essential toolkit to model and analyse spreading phenomena in complex networks. You will understand how to simulate such processes as epidemics or opinion dynamics and how to identify key spreaders of fake news or the most fragile individuals to be vaccinated in the first place.\tTak\tNie\tPoniedzia≈Çek 9.00-10.30\tPoniedzia≈Çek, 14.30-16.00\n",
    "Mateusz Nurek\tNetwork Science\tComplex networks part I - social network analysis \tComputational network science is a field of artificial intelligence that analyses graphs in applied problems involving social, transportation, epidemiological or energy issues. This workshop will teach you fundamental tools and techniques for analysing this data type. Based on a case study - the history of communication in a particular company, we will solve the problem of optimising the structure of its organisation. We will detect natural teams from employees most intensively working together. We will also identify key personnel, i.e. employees whose loss can cause communication paralysis.\t\t\tSobota, 16.30-18.00\tWtorek, 11.00-12.30\n",
    "Damian Serwata\tNetwork Science\t\"Complex networks I - social network analysis\n",
    "Complex networks II - spreading processes\"\t\tTak\tNie\tPoniedzia≈Çek, 9.00-10.30\tWtorek, 9.00-10.30\n",
    "Micha≈Ç Karol\tML w medycynie\tComputer Vision for medical image processing\tComputer vision has emerged as a revolutionary technology in the medical field, bringing significant transformations in various aspects of healthcare. Its application in clinical practice has paved the way for improved diagnostics, more accurate disease detection, and enhanced treatment planning. The objective of this workshop is to bring comprehensive understanding of the impact of computer vision in clinical practice. Participants will gain insights into how this technology is reshaping healthcare and improving patient outcomes. By exploring the latest advancements in certified medical systems, attendees will learn about the integration of computer vision into existing medical frameworks and protocols. Moreover, the workshop will delve into current research areas within computer vision in medicine. Participants will be introduced to cutting-edge studies and ongoing projects that aim to further enhance the capabilities of computer vision in the healthcare domain. In the second part of the workshop, there will be an interactive session focused on implementing classification and segmentation networks using the JAX framework and the Flax library. \tTak\tNie\tPoniedzia≈Çek, 14.30-16.00\tWtorek, 11.00-12.30\n",
    "rt Czechowski\tCyberbezpiecze≈Ñstwo\tInformatyka ≈õledcza ‚Äì wybrane zagadnienia i narzƒôdzia kryminalistyki cyfrowej \t\"Informatyka ≈õledcza pod kƒÖtem procesu analizy jest wyjƒÖtkowa pod ka≈ºdym wzglƒôdem. Opr√≥cz wiedzy teoretycznej i umiejƒôtno≈õci praktycznych wymagana jest spora wyobra≈∫nia i umiejƒôtno≈õƒá nieszablonowego podej≈õcia do ka≈ºdej analizowanej sprawy. G≈Ç√≥wnym celem informatyki ≈õledczej opr√≥cz dostarczenia cyfrowych materia≈Ç√≥w dowodowych umo≈ºliwiajƒÖcych potwierdzenie lub zaprzeczenie, i≈º dane zdarzenie mia≈Ço miejsce, jest r√≥wnie≈º przedstawienie scenariusza i toku postƒôpowania w danej sprawie (najczƒô≈õciej przygotowawczej lub postƒôpowania sƒÖdowego). G≈Ç√≥wnymi celami informatyk√≥w ≈õledczych jest ujawnienie i rekonstrukcja zdarze≈Ñ majƒÖcych charakter kryminalny, prowadzƒÖcych do zak≈Ç√≥cenia innych legalnych dzia≈Ça≈Ñ cyfrowych lub coraz czƒô≈õciej ‚Äì cyfrowych przestƒôpstw.\n",
    "\"\"\"} ]  # accumulate messages\n",
    "\n",
    "\n",
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here‚Ä¶')\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
